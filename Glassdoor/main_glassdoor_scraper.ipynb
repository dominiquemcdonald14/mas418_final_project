{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57872e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install packaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116cdd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# standard libraries\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import csv\n",
    "# 3rd-party libraries\n",
    "import enlighten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4772c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions from packaged folder\n",
    "from packaged.common import requestAndParse\n",
    "from packaged.page import extract_maximums, extract_listings\n",
    "from packaged.listing import extract_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d3d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class glassdoor_scraper():\n",
    "\n",
    "    def __init__(self, configfile, baseurl, targetnum) -> None:\n",
    "\n",
    "        # load first\n",
    "        base_url, target_num = self.load_configs(path=configfile)\n",
    "        # overwrite those that are not none\n",
    "        if type(baseurl) != type(None):\n",
    "            base_url = baseurl\n",
    "            print(\"Using supplied baseurl\")\n",
    "        if type(targetnum) != type(None):\n",
    "            target_num = targetnum\n",
    "            print(\"Using supplied targetnum\")\n",
    "        print(configfile, baseurl, targetnum)\n",
    "\n",
    "         # initialises output directory and file\n",
    "        if not os.path.exists('output'):\n",
    "            os.makedirs('output')\n",
    "        now = datetime.now() # current date and time\n",
    "        output_fileName = \"./output/output_\" + now.strftime(\"%d-%m-%Y\") + \".csv\"\n",
    "        csv_header = [(\"companyName\", \"company_starRating\", \"company_offeredRole\", \"company_roleLocation\", \"company_salary\" ,\"listing_jobDesc\", \"requested_url\")]\n",
    "        self.fileWriter(listOfTuples=csv_header, output_fileName=output_fileName) #change header here\n",
    "\n",
    "        maxJobs, maxPages = extract_maximums(base_url)\n",
    "        # print(\"[INFO] Maximum number of jobs in range: {}, number of pages in range: {}\".format(maxJobs, maxPages))\n",
    "        if (target_num >= maxJobs):\n",
    "            print(\"[ERROR] Target number larger than maximum number of jobs. Exiting program...\\n\")\n",
    "            os._exit(0)\n",
    "\n",
    "        # initialises enlighten_manager\n",
    "        enlighten_manager = enlighten.get_manager()\n",
    "        progress_outer = enlighten_manager.counter(total=target_num, desc=\"Total progress\", unit=\"listings\", color=\"green\", leave=False)\n",
    "\n",
    "        # initialise variables\n",
    "        page_index = 1\n",
    "        total_listingCount = 0\n",
    "\n",
    "        # initialises prev_url as base_url\n",
    "        prev_url = base_url\n",
    "\n",
    "        while total_listingCount <= target_num:\n",
    "            # clean up buffer\n",
    "            list_returnedTuple = []\n",
    "\n",
    "            new_url = self.update_url(prev_url, page_index)\n",
    "            page_soup,_ = requestAndParse(new_url)\n",
    "            listings_set, jobCount = extract_listings(page_soup)\n",
    "            progress_inner = enlighten_manager.counter(total=len(listings_set), desc=\"Listings scraped from page\", unit=\"listings\", color=\"blue\", leave=False)\n",
    "\n",
    "            print(\"\\n[INFO] Processing page index {}: {}\".format(page_index, new_url))\n",
    "            print(\"[INFO] Found {} links in page index {}\".format(jobCount, page_index))\n",
    "\n",
    "            for listing_url in listings_set:\n",
    "\n",
    "                # to implement cache here\n",
    "\n",
    "                returned_tuple = extract_listing(listing_url)\n",
    "                list_returnedTuple.append(returned_tuple)\n",
    "                # print(returned_tuple)\n",
    "                progress_inner.update()\n",
    "\n",
    "            progress_inner.close()\n",
    "\n",
    "            self.fileWriter(listOfTuples=list_returnedTuple, output_fileName=output_fileName)\n",
    "\n",
    "            # done with page, moving onto next page\n",
    "            total_listingCount = total_listingCount + jobCount\n",
    "            print(\"[INFO] Finished processing page index {}; Total number of jobs processed: {}\".format(page_index, total_listingCount))\n",
    "            page_index = page_index + 1\n",
    "            prev_url = new_url\n",
    "            progress_outer.update(jobCount)\n",
    "\n",
    "        progress_outer.close()    \n",
    "   \n",
    "            # loads user defined parameters\n",
    "    def load_configs(self, path):\n",
    "        with open(path) as config_file:\n",
    "            configurations = json.load(config_file)\n",
    "\n",
    "        base_url = configurations['base_url']\n",
    "        target_num = int(configurations[\"target_num\"])\n",
    "        return base_url, target_num\n",
    "\n",
    "\n",
    "    # appends list of tuples in specified output csv file\n",
    "    # a tuple is written as a single row in csv file \n",
    "    def fileWriter(self, listOfTuples, output_fileName):\n",
    "        with open(output_fileName,'a', newline='') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            for row_tuple in listOfTuples:\n",
    "                try:\n",
    "                    csv_out.writerow(row_tuple)\n",
    "                    # can also do csv_out.writerows(data) instead of the for loop\n",
    "                except Exception as e:\n",
    "                    print(\"[WARN] In filewriter: {}\".format(e))\n",
    "\n",
    "\n",
    "    # updates url according to the page_index desired\n",
    "    def update_url(self, prev_url, page_index):\n",
    "        if page_index == 1:\n",
    "            prev_substring = \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "        else:\n",
    "            prev_substring = \"_IP\" + str(page_index - 1) + \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "\n",
    "        new_url = prev_url.replace(prev_substring, new_substring)\n",
    "        return new_url\n",
    "#Ask Angel...\n",
    "#if __name__ == \"__main__\":\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument('-c', '--configfile', help=\"Specify location of json config file\", type=str, required=False, default=\"config.json\")\n",
    "#    parser.add_argument('-b', '--baseurl', help=\"Base_url to use. Overwrites config file\", type=str, required=False, default=None)\n",
    "#    parser.add_argument('-tn', '--targetnum', help=\"Target number to scrape. Overwrites config file\", type=int, required=False, default=None)\n",
    "#    args = vars(parser.parse_args())\n",
    "    \n",
    "    \n",
    "\n",
    "#    glassdoor_scraper( \n",
    "#        configfile=args[\"configfile\"],\n",
    "#        baseurl=args[\"baseurl\"],\n",
    "#        targetnum=args[\"targetnum\"]\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f947ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using supplied baseurl\n",
      "Using supplied targetnum\n",
      "config.json https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm?context=Jobs&clickSource=searchBox 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".enlighten-fg-green {\n",
       "  color: #00cd00;\n",
       "}\n",
       ".enlighten-fg-blue {\n",
       "  color: #0000ee;\n",
       "}\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Total progress 30 listings [01:45, 0.29 listings/s]                                                 </pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Listings scraped from page 100%|<span class=\"enlighten-fg-blue\">||||||||||||||||||||||||||||||</span>| 30/30 [01:43&lt;00:00, 0.29 listings/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing page index 1: https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP1.htm?context=Jobs&clickSource=searchBox\n",
      "[INFO] Found 30 links in page index 1\n",
      "[INFO] Finished processing page index 1; Total number of jobs processed: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.glassdoor_scraper at 0x222cb5e0400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glassdoor_scraper(configfile = \"config.json\",baseurl = \"https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm?context=Jobs&clickSource=searchBox\", targetnum = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
