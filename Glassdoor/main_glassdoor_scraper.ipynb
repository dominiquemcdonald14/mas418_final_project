{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116cdd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# standard libraries\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import csv\n",
    "# 3rd-party libraries\n",
    "#import enlighten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4772c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions from packaged folder\n",
    "from packaged.common import requestAndParse\n",
    "from packaged.page import extract_maximums, extract_listings\n",
    "from packaged.listing import extract_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d3d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class glassdoor_scraper():\n",
    "\n",
    "    def __init__(self, configfile, baseurl, targetnum) -> None:\n",
    "\n",
    "        # load first\n",
    "        base_url, target_num = self.load_configs(path=configfile)\n",
    "        # overwrite those that are not none\n",
    "        if type(baseurl) != type(None):\n",
    "            base_url = baseurl\n",
    "            print(\"Using supplied baseurl\")\n",
    "        if type(targetnum) != type(None):\n",
    "            target_num = targetnum\n",
    "            print(\"Using supplied targetnum\")\n",
    "        print(configfile, baseurl, targetnum)\n",
    "\n",
    "         # initialises output directory and file\n",
    "        if not os.path.exists('output'):\n",
    "            os.makedirs('output')\n",
    "        now = datetime.now() # current date and time\n",
    "        output_fileName = \"./output/output_\" + now.strftime(\"%m-%d-%Y\") + \".csv\"\n",
    "        csv_header = [(\"companyName\", \"company_starRating\", \"company_offeredRole\", \"company_roleLocation\", \"company_salary\" ,\"listing_jobDesc\", \"requested_url\")]\n",
    "        self.fileWriter(listOfTuples=csv_header, output_fileName=output_fileName) #change header here\n",
    "\n",
    "        maxJobs, maxPages = extract_maximums(base_url)\n",
    "        # print(\"[INFO] Maximum number of jobs in range: {}, number of pages in range: {}\".format(maxJobs, maxPages))\n",
    "        if (target_num >= maxJobs):\n",
    "            print(\"[ERROR] Target number larger than maximum number of jobs. Exiting program...\\n\")\n",
    "            os._exit(0)\n",
    "\n",
    "        # initialises enlighten_manager\n",
    "        enlighten_manager = enlighten.get_manager()\n",
    "        progress_outer = enlighten_manager.counter(total=target_num, desc=\"Total progress\", unit=\"listings\", color=\"green\", leave=False)\n",
    "\n",
    "        # initialise variables\n",
    "        page_index = 1\n",
    "        total_listingCount = 0\n",
    "\n",
    "        # initialises prev_url as base_url\n",
    "        prev_url = base_url\n",
    "\n",
    "        while total_listingCount <= target_num:\n",
    "            # clean up buffer\n",
    "            list_returnedTuple = []\n",
    "\n",
    "            new_url = self.update_url(prev_url, page_index)\n",
    "            page_soup,_ = requestAndParse(new_url)\n",
    "            listings_set, jobCount = extract_listings(page_soup)\n",
    "            progress_inner = enlighten_manager.counter(total=len(listings_set), desc=\"Listings scraped from page\", unit=\"listings\", color=\"blue\", leave=False)\n",
    "\n",
    "            print(\"\\n[INFO] Processing page index {}: {}\".format(page_index, new_url))\n",
    "            print(\"[INFO] Found {} links in page index {}\".format(jobCount, page_index))\n",
    "\n",
    "            for listing_url in listings_set:\n",
    "\n",
    "                # to implement cache here\n",
    "\n",
    "                returned_tuple = extract_listing(listing_url)\n",
    "                list_returnedTuple.append(returned_tuple)\n",
    "                # print(returned_tuple)\n",
    "                progress_inner.update()\n",
    "\n",
    "            progress_inner.close()\n",
    "\n",
    "            self.fileWriter(listOfTuples=list_returnedTuple, output_fileName=output_fileName)\n",
    "\n",
    "            # done with page, moving onto next page\n",
    "            total_listingCount = total_listingCount + jobCount\n",
    "            print(\"[INFO] Finished processing page index {}; Total number of jobs processed: {}\".format(page_index, total_listingCount))\n",
    "            page_index = page_index + 1\n",
    "            prev_url = new_url\n",
    "            progress_outer.update(jobCount)\n",
    "\n",
    "        progress_outer.close()    \n",
    "   \n",
    "            # loads user defined parameters\n",
    "    def load_configs(self, path):\n",
    "        with open(path) as config_file:\n",
    "            configurations = json.load(config_file)\n",
    "\n",
    "        base_url = configurations['base_url']\n",
    "        target_num = int(configurations[\"target_num\"])\n",
    "        return base_url, target_num\n",
    "\n",
    "\n",
    "    # appends list of tuples in specified output csv file\n",
    "    # a tuple is written as a single row in csv file \n",
    "    def fileWriter(self, listOfTuples, output_fileName):\n",
    "        with open(output_fileName,'a', newline='') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            for row_tuple in listOfTuples:\n",
    "                try:\n",
    "                    csv_out.writerow(row_tuple)\n",
    "                    # can also do csv_out.writerows(data) instead of the for loop\n",
    "                except Exception as e:\n",
    "                    print(\"[WARN] In filewriter: {}\".format(e))\n",
    "\n",
    "\n",
    "    # updates url according to the page_index desired\n",
    "    def update_url(self, prev_url, page_index):\n",
    "        if page_index == 1:\n",
    "            prev_substring = \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "        else:\n",
    "            prev_substring = \"_IP\" + str(page_index - 1) + \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "\n",
    "        new_url = prev_url.replace(prev_substring, new_substring)\n",
    "        return new_url\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument('-c', '--configfile', help=\"Specify location of json config file\", type=str, required=False, default=\"config.json\")\n",
    "#    parser.add_argument('-b', '--baseurl', help=\"Base_url to use. Overwrites config file\", type=str, required=False, default=None)\n",
    "#    parser.add_argument('-tn', '--targetnum', help=\"Target number to scrape. Overwrites config file\", type=int, required=False, default=None)\n",
    "#    args = vars(parser.parse_args())\n",
    "    \n",
    "    \n",
    "\n",
    "#    glassdoor_scraper( \n",
    "#        configfile=args[\"configfile\"],\n",
    "#        baseurl=args[\"baseurl\"],\n",
    "#        targetnum=args[\"targetnum\"]\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f947ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using supplied baseurl\n",
      "Using supplied targetnum\n",
      "config.json https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm?context=Jobs&clickSource=searchBox 10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'enlighten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49936\\3967802967.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglassdoor_scraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"config.json\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbaseurl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm?context=Jobs&clickSource=searchBox\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49936\\3421433908.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, configfile, baseurl, targetnum)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# initialises enlighten_manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0menlighten_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menlighten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mprogress_outer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menlighten_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Total progress\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"listings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"green\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'enlighten' is not defined"
     ]
    }
   ],
   "source": [
    "glassdoor_scraper(configfile = \"config.json\",baseurl = \"https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm?context=Jobs&clickSource=searchBox\", targetnum = 900)\n",
    "#request to scrape the maximum amount of jobs listed on glassdoor website = 900 (30 listings per page, 30 pages total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
