{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8666d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# standard libraries\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import csv\n",
    "# 3rd-party libraries\n",
    "import enlighten\n",
    "\n",
    "# custom functions from packaged folder\n",
    "from packaged.common import requestAndParse\n",
    "from packaged.page import extract_maximums, extract_listings\n",
    "from packaged.listing import extract_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9039193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class glassdoor_scraper():\n",
    "\n",
    "    def __init__(self, configfile, baseurl, targetnum) -> None:\n",
    "\n",
    "        # load first\n",
    "        base_url, target_num = self.load_configs(path=configfile)\n",
    "        # overwrite those that are not none\n",
    "        if type(baseurl) != type(None):\n",
    "            base_url = baseurl\n",
    "            print(\"Using supplied baseurl\")\n",
    "        if type(targetnum) != type(None):\n",
    "            target_num = targetnum\n",
    "            print(\"Using supplied targetnum\")\n",
    "        print(configfile, baseurl, targetnum)\n",
    "        \n",
    "        if not os.path.exists('output'):\n",
    "            os.makedirs('output')\n",
    "        now = datetime.now() # current date and time\n",
    "        output_fileName = \"./output/output_\" + \".csv\" #+ now.strftime(\"%d-%m-%Y\")\n",
    "        csv_header = [(\"companyName\", \"company_starRating\", \"company_offeredRole\", \"company_roleLocation\", \"company_salary\" , \"listing_jobDesc\", \"requested_url\")]\n",
    "        self.fileWriter(listOfTuples=csv_header, output_fileName=output_fileName)\n",
    "        \n",
    "        if os.path.exists('output'):\n",
    "            output_fileName = \"./output/output_\" + \".csv\"\n",
    "            os.remove(output_fileName)\n",
    "        \n",
    "         # initialises output directory and file\n",
    "        if not os.path.exists('output'):\n",
    "            os.makedirs('output')\n",
    "        now = datetime.now() # current date and time\n",
    "        output_fileName = \"./output/output_\" + \".csv\" #+ now.strftime(\"%d-%m-%Y\") I changed the name of our output\n",
    "        csv_header = [(\"companyName\", \"company_starRating\", \"company_offeredRole\", \"company_roleLocation\", \"company_salary\" , \"listing_jobDesc\", \"requested_url\")]\n",
    "        self.fileWriter(listOfTuples=csv_header, output_fileName=output_fileName) #change header here\n",
    "\n",
    "        maxJobs, maxPages = extract_maximums(base_url)\n",
    "        # print(\"[INFO] Maximum number of jobs in range: {}, number of pages in range: {}\".format(maxJobs, maxPages))\n",
    "        if (target_num >= maxJobs):\n",
    "            print(\"[ERROR] Target number larger than maximum number of jobs. Exiting program...\\n\")\n",
    "            os._exit(0)\n",
    "\n",
    "        # initialises enlighten_manager\n",
    "        enlighten_manager = enlighten.get_manager()\n",
    "        progress_outer = enlighten_manager.counter(total=target_num, desc=\"Total progress\", unit=\"listings\", color=\"green\", leave=False)\n",
    "# initialise variables\n",
    "        page_index = 1\n",
    "        total_listingCount = 0\n",
    "\n",
    "        # initialises prev_url as base_url\n",
    "        prev_url = base_url\n",
    "\n",
    "        while total_listingCount <= target_num:\n",
    "            # clean up buffer\n",
    "            list_returnedTuple = []\n",
    "\n",
    "            new_url = self.update_url(prev_url, page_index)\n",
    "            page_soup,_ = requestAndParse(new_url)\n",
    "            listings_set, jobCount = extract_listings(page_soup)\n",
    "            progress_inner = enlighten_manager.counter(total=len(listings_set), desc=\"Listings scraped from page\", unit=\"listings\", color=\"blue\", leave=False)\n",
    "\n",
    "            print(\"\\n[INFO] Processing page index {}: {}\".format(page_index, new_url))\n",
    "            print(\"[INFO] Found {} links in page index {}\".format(jobCount, page_index))\n",
    "\n",
    "            for listing_url in listings_set:\n",
    "\n",
    "                # to implement cache here\n",
    "\n",
    "                returned_tuple = extract_listing(listing_url)\n",
    "                list_returnedTuple.append(returned_tuple)\n",
    "                # print(returned_tuple)\n",
    "                progress_inner.update()\n",
    "\n",
    "            progress_inner.close()\n",
    "\n",
    "            self.fileWriter(listOfTuples=list_returnedTuple, output_fileName=output_fileName)\n",
    "\n",
    "            # done with page, moving onto next page\n",
    "            total_listingCount = total_listingCount + jobCount\n",
    "            print(\"[INFO] Finished processing page index {}; Total number of jobs processed: {}\".format(page_index, total_listingCount))\n",
    "            page_index = page_index + 1\n",
    "            prev_url = new_url\n",
    "            progress_outer.update(jobCount)\n",
    "\n",
    "        progress_outer.close()\n",
    "\n",
    "            # loads user defined parameters\n",
    "    def load_configs(self, path):\n",
    "        with open(path) as config_file:\n",
    "            configurations = json.load(config_file)\n",
    "\n",
    "        base_url = configurations['base_url']\n",
    "        target_num = int(configurations[\"target_num\"])\n",
    "        return base_url, target_num\n",
    "\n",
    "\n",
    "    # appends list of tuples in specified output csv file\n",
    "    # a tuple is written as a single row in csv file \n",
    "    def fileWriter(self, listOfTuples, output_fileName):\n",
    "        with open(output_fileName,'a', newline='') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            for row_tuple in listOfTuples:\n",
    "                try:\n",
    "                    csv_out.writerow(row_tuple)\n",
    "                    # can also do csv_out.writerows(data) instead of the for loop\n",
    "                except Exception as e:\n",
    "                    print(\"[WARN] In filewriter: {}\".format(e))\n",
    "\n",
    "\n",
    "    # updates url according to the page_index desired\n",
    "    def update_url(self, prev_url, page_index):\n",
    "        if page_index == 1:\n",
    "            prev_substring = \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "        else:\n",
    "            prev_substring = \"_IP\" + str(page_index - 1) + \".htm\"\n",
    "            new_substring = \"_IP\" + str(page_index) + \".htm\"\n",
    "\n",
    "        new_url = prev_url.replace(prev_substring, new_substring)\n",
    "        return new_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d13e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using supplied baseurl\n",
      "Using supplied targetnum\n",
      "config.json https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm?context=Jobs&clickSource=searchBox 90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".enlighten-fg-green {\n",
       "  color: #00cd00;\n",
       "}\n",
       ".enlighten-fg-blue {\n",
       "  color: #0000ee;\n",
       "}\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Total progress 120 listings [04:37, 0.43 listings/s]                                                </pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Listings scraped from page 100%|<span class=\"enlighten-fg-blue\">██████████████████████████████</span>| 30/30 [01:17&lt;00:00, 0.39 listings/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing page index 1: https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP1.htm?context=Jobs&clickSource=searchBox\n",
      "[INFO] Found 30 links in page index 1\n",
      "HTTP Error 503: Service Unavailable\n",
      "[ERROR] Error occurred in extract_listing, requested url: www.glassdoor.com/partner/jobListing.htm?pos=104&ao=1110586&s=58&guid=00000186c8a4be3a9dc9a130911e27a8&src=GD_JOB_AD&t=SR&vt=w&cs=1_14920d27&cb=1678403485659&jobListingId=1008515047043&cpc=ACBF47B84C432121&jrtk=3-0-1gr4a9fjejord801-1gr4a9fk4harn800-8654a73e7c43a980--6NYlbfkN0DeKde-pU_olD4YUrw_gjyRI1n-4QX0HDStiN2-PsFXQbNScsEQqavu_Pyy036DCeMMLyaJ8bpvi2Pa6H4Jus1-YaAZPLT3S5HxW2qkGbOw6wmsm0MrxQ7DVDBCcbvn0sCSdnwlmRwVFYZ_oeeH7MyoErtvYtZWCsh0-pMViGuaXsJkAf31lU1WtrZx6qFHH2qLdb2w0X6b5igoPzs3eBse8xblfZkIDyhCZ4QorFBzIcFtwfs8aMTWq-5TBxuu338z6HdaVBRTo0xb2IU7uzVtv1EuTnue_i525RocXsJfj1ELuFHNiiK9LSMQujvF33szs8Ua_k1dVYrzC1tDFfiz-IxpJc1IqyoGhL6ZzC13nrvPWfS2-cG5pRcYthyuMwGUPS41M_zkYrTi7X8DTk9b64Nc4TiYWTHVWWGmJBx8L6oBLC6NKq0SPcKqzGG7oaluKEcqLrybN81WMuP-7p0vLGiLHA-2AlsYnBO3CldIClbl_fwlwNnV4QV6yKOEm7bcbTHmZLtkSimWsha46-UZyJ-gCPxFAovcLJT55fq_MKY3wqHmqlvoDsYowlyZuYmI2jokXEbF26qCMe5BsLhiaDQA-utWOY6sqVxWZPxFfjFBbVCZoltIYFh-w7X3m_rMNyGvh1tLCOUZnIipzPvXCBdgzeaJQ3Kw_FYOmslj9dzjPvcqMVdcKbLmZGUvHoXJpj1yKua3zGf3_2WXlKO8hDrsVdE4CmU%3D is unavailable.\n",
      "[INFO] Finished processing page index 1; Total number of jobs processed: 30\n",
      "\n",
      "[INFO] Processing page index 2: https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP2.htm?context=Jobs&clickSource=searchBox\n",
      "[INFO] Found 30 links in page index 2\n",
      "[INFO] Finished processing page index 2; Total number of jobs processed: 60\n",
      "\n",
      "[INFO] Processing page index 3: https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP3.htm?context=Jobs&clickSource=searchBox\n",
      "[INFO] Found 30 links in page index 3\n",
      "[INFO] Finished processing page index 3; Total number of jobs processed: 90\n",
      "\n",
      "[INFO] Processing page index 4: https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP4.htm?context=Jobs&clickSource=searchBox\n",
      "[INFO] Found 30 links in page index 4\n",
      "[INFO] Finished processing page index 4; Total number of jobs processed: 120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.glassdoor_scraper at 0x7fc76053b1f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the scraper  \n",
    "glassdoor_scraper(configfile = \"config.json\",\n",
    "                  baseurl = \"https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm?context=Jobs&clickSource=searchBox\", \n",
    "                  targetnum = 90) #for the cleanup, working with a small dataset 90 observations\n",
    "#request to scrape the maximum amount of jobs listed on glassdoor website = 900 (30 listings per page, 30 pages total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
